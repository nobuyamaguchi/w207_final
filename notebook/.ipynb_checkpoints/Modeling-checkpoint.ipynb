{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from matplotlib.ticker import MultipleLocator\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn import preprocessing\n",
    "\n",
    "\n",
    "# SK-learn libraries for evaluation.\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "%matplotlib inline\n",
    "import time\n",
    "\n",
    "import theano.tensor\n",
    "from theano import tensor as T\n",
    "from theano.sandbox.rng_mrg import MRG_RandomStreams as RandomStreams\n",
    "from keras import optimizers\n",
    "from keras.models import Sequential \n",
    "from keras.layers import Dense, Activation, Dropout\n",
    "from keras.layers.convolutional import Conv2D\n",
    "from keras.layers.convolutional import MaxPooling2D\n",
    "from keras.layers import Flatten\n",
    "from keras.utils import np_utils\n",
    "from keras.datasets import mnist\n",
    "from keras import backend as K\n",
    "\n",
    "np.random.seed(0)\n",
    "print (\"OK\")\n",
    "\n",
    "# Set the randomizer seed so results are the same each time.\n",
    "np.random.seed(0)\n",
    "\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data set shape: (15120, 53)\n",
      "training label shape: (9072,)\n",
      "dev label shape: (3024,)\n",
      "test label shape: (3024,)\n",
      "labels names: ['1. Spruce/Fir', '2. Lodgepole Pine', '3. Ponderosa Pine', '4. Cottonwood/Willow', '5. Aspen', '6. Douglas-fir', '7. Krummholz']\n",
      "number of features: 52\n",
      "feature names:\n",
      " ['Elevation', 'Aspect', 'Slope', 'Horizontal_Distance_To_Hydrology', 'Vertical_Distance_To_Hydrology', 'Horizontal_Distance_To_Roadways', 'Hillshade_9am', 'Hillshade_Noon', 'Hillshade_3pm', 'Horizontal_Distance_To_Fire_Points', 'Wilderness_Area1', 'Wilderness_Area2', 'Wilderness_Area3', 'Wilderness_Area4', 'Soil_Type1', 'Soil_Type2', 'Soil_Type3', 'Soil_Type4', 'Soil_Type5', 'Soil_Type6', 'Soil_Type8', 'Soil_Type9', 'Soil_Type10', 'Soil_Type11', 'Soil_Type12', 'Soil_Type13', 'Soil_Type14', 'Soil_Type16', 'Soil_Type17', 'Soil_Type18', 'Soil_Type19', 'Soil_Type20', 'Soil_Type21', 'Soil_Type22', 'Soil_Type23', 'Soil_Type24', 'Soil_Type25', 'Soil_Type26', 'Soil_Type27', 'Soil_Type28', 'Soil_Type29', 'Soil_Type30', 'Soil_Type31', 'Soil_Type32', 'Soil_Type33', 'Soil_Type34', 'Soil_Type35', 'Soil_Type36', 'Soil_Type37', 'Soil_Type38', 'Soil_Type39', 'Soil_Type40']\n"
     ]
    }
   ],
   "source": [
    "Forest_data = pd.read_csv(\"../data/train.csv\") \n",
    "Forest_data.pop('Id')\n",
    "Forest_data.pop('Soil_Type7')\n",
    "Forest_data.pop('Soil_Type15')\n",
    "print('data set shape:', Forest_data.shape)\n",
    "\n",
    "Cover_Types = ['1. Spruce/Fir', '2. Lodgepole Pine', '3. Ponderosa Pine', \n",
    "               '4. Cottonwood/Willow', '5. Aspen', '6. Douglas-fir', '7. Krummholz']\n",
    "\n",
    "# Train, dev, test split (70/20/10)\n",
    "split1 = int(len(Forest_data)* 0.60)\n",
    "split2 = int(split1 + (len(Forest_data) - split1) / 2)\n",
    "\n",
    "train_data, train_labels = Forest_data[:split1].drop(columns=['Cover_Type']), Forest_data.Cover_Type[:split1]\n",
    "dev_data, dev_labels     = Forest_data[split1:split2].drop(columns=['Cover_Type']), Forest_data.Cover_Type[split1:split2]\n",
    "test_data, test_labels   = Forest_data[split2:].drop(columns=['Cover_Type']), Forest_data.Cover_Type[split2:]\n",
    "\n",
    "print('training label shape:', train_labels.shape)\n",
    "print('dev label shape:',      dev_labels.shape)\n",
    "print('test label shape:',     test_labels.shape)\n",
    "print('labels names:',         Cover_Types)\n",
    "print('number of features:',   len(train_data.columns))\n",
    "print('feature names:\\n',   list(train_data.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Pre-processing - Normalize variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Get column names first\n",
    "to_normalize = ['Elevation', 'Aspect', 'Slope', 'Horizontal_Distance_To_Hydrology', 'Vertical_Distance_To_Hydrology', 'Horizontal_Distance_To_Roadways', 'Hillshade_9am', 'Hillshade_Noon', 'Hillshade_3pm', 'Horizontal_Distance_To_Fire_Points']\n",
    "Binary_features = ['Wilderness_Area1', 'Wilderness_Area2', 'Wilderness_Area3', 'Wilderness_Area4', 'Soil_Type1', 'Soil_Type2', 'Soil_Type3', 'Soil_Type4', 'Soil_Type5', 'Soil_Type6', 'Soil_Type8', 'Soil_Type9', 'Soil_Type10', 'Soil_Type11', 'Soil_Type12', 'Soil_Type13', 'Soil_Type14', 'Soil_Type16', 'Soil_Type17', 'Soil_Type18', 'Soil_Type19', 'Soil_Type20', 'Soil_Type21', 'Soil_Type22', 'Soil_Type23', 'Soil_Type24', 'Soil_Type25', 'Soil_Type26', 'Soil_Type27', 'Soil_Type28', 'Soil_Type29', 'Soil_Type30', 'Soil_Type31', 'Soil_Type32', 'Soil_Type33', 'Soil_Type34', 'Soil_Type35', 'Soil_Type36', 'Soil_Type37', 'Soil_Type38', 'Soil_Type39', 'Soil_Type40']\n",
    "scaled_train_data = pd.concat([pd.DataFrame(preprocessing.scale(train_data[to_normalize]),columns=to_normalize),\n",
    "                              train_data[Binary_features]], axis=1,ignore_index=True)\n",
    "scaled_dev_data = pd.concat([pd.DataFrame(preprocessing.scale(dev_data[to_normalize]),columns=to_normalize),\n",
    "                              dev_data.reset_index()[Binary_features]], axis=1, ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "When C=0.001, Logistic Regression Model accuracy: 0.53 f1_score: 0.55\n",
      "When C=0.01, Logistic Regression Model accuracy: 0.59 f1_score: 0.59\n",
      "When C=0.1, Logistic Regression Model accuracy: 0.63 f1_score: 0.64\n",
      "When C=0.5, Logistic Regression Model accuracy: 0.62 f1_score: 0.63\n",
      "When C=1, Logistic Regression Model accuracy: 0.63 f1_score: 0.64\n",
      "When C=10, Logistic Regression Model accuracy: 0.65 f1_score: 0.66\n",
      "When C=100, Logistic Regression Model accuracy: 0.61 f1_score: 0.62\n",
      "When C=1000, Logistic Regression Model accuracy: 0.62 f1_score: 0.63\n"
     ]
    }
   ],
   "source": [
    "L2_strengths = [0.001, 0.01, 0.1, 0.5, 1, 10, 100, 1000]\n",
    "for c in L2_strengths:\n",
    "    model = LogisticRegression(C=c, solver=\"liblinear\", multi_class=\"auto\", penalty=\"l2\")\n",
    "    model.fit(train_data, train_labels)\n",
    "    test_predicted_labels = model.predict(dev_data)\n",
    "    print('When C=' + str(c) + ', Logistic Regression Model accuracy: %3.2f f1_score: %3.2f' \n",
    "           %(model.score(dev_data, dev_labels), \n",
    "             metrics.f1_score(dev_labels, test_predicted_labels, average=\"weighted\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegression(C=10, solver=\"liblinear\", multi_class=\"auto\", penalty=\"l2\")\n",
    "model.fit(train_data[new_features], train_labels)\n",
    "test_predicted_labels = model.predict(dev_data[new_features])\n",
    "print('When C=' + str(10) + ', Logistic Regression Model accuracy: %3.2f f1_score: %3.2f' \n",
    "       %(model.score(dev_data[new_features], dev_labels), \n",
    "         metrics.f1_score(dev_labels, test_predicted_labels, average=\"weighted\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Use L2 strength C=10 going forward."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Selection\n",
    "> Produce a Logistic Regression model using the L1 regularization strength. Reduce the features to only those have at least one non-zero weight among the four categories. Produce a new Logistic Regression model using the reduced vocabulary and L2 regularization strength of 10. Whereas L2 regularization makes all the weights relatively small, L1 regularization drives many of the weights to 0, effectively removing unimportant features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Each point corresponds to a specific L1 regularization strength used to reduce the vocabulary.\n",
    "L1_regularization_strength = [0.01, 0.05, 0.1, 0.5, 1, 5, 10]\n",
    "f1_score = []\n",
    "\n",
    "for c in L1_regularization_strength:\n",
    "    # Produce a Logistic Regression model using the L1 regularization strength\n",
    "    model_L1_regularization = LogisticRegression(solver=\"liblinear\", multi_class=\"auto\", penalty=\"l1\", C=c, tol=0.015)\n",
    "    model_L1_regularization.fit(train_data, train_labels)\n",
    "\n",
    "    # Reduce the vocabulary to only those features that have at least one non-zero weight among the four categories.\n",
    "    old_features = np.array(train_data.columns)\n",
    "    new_features = old_features[model_L1_regularization.coef_.sum(axis = 0) != 0]\n",
    "\n",
    "    model = LogisticRegression(C=10, solver=\"liblinear\", multi_class=\"auto\", penalty=\"l2\")\n",
    "    model.fit(train_data[new_features], train_labels)\n",
    "    test_predicted_labels = model.predict(dev_data[new_features])\n",
    "    print('When C=' + str(c) + ', Logistic Regression Model accuracy: %3.2f f1_score: %3.2f' \n",
    "           %(model.score(dev_data[new_features], dev_labels), \n",
    "             metrics.f1_score(dev_labels, test_predicted_labels, average=\"weighted\")))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* F1 score didn't improve much in this process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Improvement ideas:**\n",
    "* Normalize some coefficients\n",
    "* Try other models as Logistic Regression is not good at predicting multiple classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do a 3-way split for error analysis "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neuro Network - Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1-of-n encoding\n",
    "Make a set of 7 binary values, one for each class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes = 7\n",
      "Features = 52\n"
     ]
    }
   ],
   "source": [
    "def binarizeY(data):\n",
    "    binarized_data = np.zeros((data.size,7))\n",
    "    for j in range(0,data.size):\n",
    "        feature = data[j:j+1]\n",
    "        i = feature.astype(np.int64) \n",
    "        binarized_data[j,i-1]=1\n",
    "    return binarized_data\n",
    "train_labels_b = binarizeY(train_labels)\n",
    "dev_labels_b = binarizeY(dev_labels)\n",
    "numClasses = train_labels_b[1].size\n",
    "print('Classes = %d'%(numClasses))\n",
    "numFeatures = train_data.shape[1]\n",
    "print('Features = %d'%(numFeatures))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test score: 1.810789809655891\n",
      "Test accuracy: 0.22089947760105133\n"
     ]
    }
   ],
   "source": [
    "## Model\n",
    "model = Sequential() \n",
    "model.add(Dense(numClasses, input_dim=numFeatures, activation='softmax')) \n",
    "\n",
    "## Cost function & Objective (and solver)\n",
    "sgd = optimizers.SGD(lr=0.01)\n",
    "model.compile(optimizer=sgd, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "history = model.fit(scaled_train_data, train_labels_b, shuffle=False, batch_size=scaled_train_data.shape[0],verbose=0, epochs=200) \n",
    "score = model.evaluate(scaled_dev_data, dev_labels_b, verbose=0) \n",
    "print('Test score:', score[0]) \n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Theano"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1) accuracy = 0.4458\n",
      "2) accuracy = 0.5152\n",
      "3) accuracy = 0.5208\n",
      "4) accuracy = 0.5222\n",
      "5) accuracy = 0.5248\n",
      "6) accuracy = 0.5208\n",
      "7) accuracy = 0.5169\n",
      "8) accuracy = 0.5152\n",
      "9) accuracy = 0.5165\n",
      "10) accuracy = 0.5146\n",
      "11) accuracy = 0.5142\n",
      "12) accuracy = 0.5119\n",
      "13) accuracy = 0.5106\n",
      "14) accuracy = 0.5099\n",
      "15) accuracy = 0.5086\n",
      "16) accuracy = 0.5050\n",
      "17) accuracy = 0.5013\n",
      "18) accuracy = 0.4974\n",
      "19) accuracy = 0.4954\n",
      "20) accuracy = 0.4901\n",
      "21) accuracy = 0.4851\n",
      "22) accuracy = 0.4821\n",
      "23) accuracy = 0.4821\n",
      "24) accuracy = 0.4805\n",
      "25) accuracy = 0.4792\n",
      "26) accuracy = 0.4769\n",
      "27) accuracy = 0.4752\n",
      "28) accuracy = 0.4719\n",
      "29) accuracy = 0.4709\n",
      "30) accuracy = 0.4706\n",
      "31) accuracy = 0.4699\n",
      "32) accuracy = 0.4676\n",
      "33) accuracy = 0.4656\n",
      "34) accuracy = 0.4646\n",
      "35) accuracy = 0.4640\n",
      "36) accuracy = 0.4620\n",
      "37) accuracy = 0.4610\n",
      "38) accuracy = 0.4597\n",
      "39) accuracy = 0.4593\n",
      "40) accuracy = 0.4587\n",
      "41) accuracy = 0.4573\n",
      "42) accuracy = 0.4570\n",
      "43) accuracy = 0.4563\n",
      "44) accuracy = 0.4550\n",
      "45) accuracy = 0.4540\n",
      "46) accuracy = 0.4517\n",
      "47) accuracy = 0.4511\n",
      "48) accuracy = 0.4501\n",
      "49) accuracy = 0.4507\n",
      "50) accuracy = 0.4504\n",
      "train time = 7634.36\n",
      "predict time = 0.00\n"
     ]
    }
   ],
   "source": [
    "## (1) Parameters\n",
    "w = theano.shared(np.asarray((np.random.randn(*(numFeatures, numClasses))*.01)))\n",
    "\n",
    "## (2) Model\n",
    "X = T.matrix()\n",
    "Y = T.matrix()\n",
    "def model(X, w):\n",
    "    return T.nnet.softmax(T.dot(X, w))\n",
    "y_hat = model(X, w)\n",
    "\n",
    "## (3) Cost\n",
    "cost = T.mean(T.nnet.categorical_crossentropy(y_hat, Y))\n",
    "\n",
    "## (4) Objective\n",
    "alpha = 0.01\n",
    "gradient = T.grad(cost=cost, wrt=w)\n",
    "update = [[w, w - gradient * alpha]] \n",
    "train = theano.function(inputs=[X, Y], outputs=cost, updates=update, allow_input_downcast=True) \n",
    "y_pred = T.argmax(y_hat, axis=1) \n",
    "predict = theano.function(inputs=[X], outputs=y_pred, allow_input_downcast=True)\n",
    "\n",
    "miniBatchSize = 1 \n",
    "def gradientDescentStochastic(epochs):\n",
    "    trainTime = 0.0\n",
    "    predictTime = 0.0\n",
    "    start_time = time.time()\n",
    "    for i in range(epochs):       \n",
    "        for start, end in zip(range(0, len(scaled_train_data), miniBatchSize), range(miniBatchSize, len(scaled_train_data), miniBatchSize)):\n",
    "            cost = train(scaled_train_data[start:end], train_labels_b[start:end])\n",
    "        trainTime =  trainTime + (time.time() - start_time)\n",
    "        print('%d) accuracy = %.4f' %(i+1, np.mean(np.argmax(dev_labels_b, axis=1) == predict(scaled_dev_data))))     \n",
    "    print('train time = %.2f' %(trainTime))\n",
    "    \n",
    "gradientDescentStochastic(50)\n",
    "\n",
    "start_time = time.time()\n",
    "predict(scaled_dev_data)   \n",
    "print('predict time = %.2f'%(time.time() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-layer Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1) accuracy = 0.3353\n",
      "2) accuracy = 0.3495\n",
      "3) accuracy = 0.3271\n",
      "4) accuracy = 0.3095\n",
      "5) accuracy = 0.3032\n",
      "6) accuracy = 0.3039\n",
      "7) accuracy = 0.3059\n",
      "8) accuracy = 0.3112\n",
      "9) accuracy = 0.3194\n",
      "10) accuracy = 0.3300\n",
      "11) accuracy = 0.3366\n",
      "12) accuracy = 0.3485\n",
      "13) accuracy = 0.3591\n",
      "14) accuracy = 0.3624\n",
      "15) accuracy = 0.3654\n",
      "16) accuracy = 0.3644\n",
      "17) accuracy = 0.3595\n",
      "18) accuracy = 0.3575\n",
      "19) accuracy = 0.3558\n",
      "20) accuracy = 0.3528\n",
      "21) accuracy = 0.3485\n",
      "22) accuracy = 0.3442\n",
      "23) accuracy = 0.3380\n",
      "24) accuracy = 0.3347\n",
      "25) accuracy = 0.3343\n",
      "26) accuracy = 0.3307\n",
      "27) accuracy = 0.3287\n",
      "28) accuracy = 0.3280\n",
      "29) accuracy = 0.3271\n",
      "30) accuracy = 0.3274\n",
      "31) accuracy = 0.3267\n",
      "32) accuracy = 0.3264\n",
      "33) accuracy = 0.3277\n",
      "34) accuracy = 0.3267\n",
      "35) accuracy = 0.3287\n",
      "36) accuracy = 0.3277\n",
      "37) accuracy = 0.3290\n",
      "38) accuracy = 0.3300\n",
      "39) accuracy = 0.3323\n",
      "40) accuracy = 0.3337\n",
      "41) accuracy = 0.3350\n",
      "42) accuracy = 0.3356\n",
      "43) accuracy = 0.3370\n",
      "44) accuracy = 0.3373\n",
      "45) accuracy = 0.3380\n",
      "46) accuracy = 0.3390\n",
      "47) accuracy = 0.3386\n",
      "48) accuracy = 0.3419\n",
      "49) accuracy = 0.3442\n",
      "50) accuracy = 0.3452\n",
      "train time = 10870.33\n",
      "predict time = 0.02\n"
     ]
    }
   ],
   "source": [
    "## (1) Parameters\n",
    "numHiddenNodes = 600 \n",
    "w_1 = theano.shared(np.asarray((np.random.randn(*(numFeatures, numHiddenNodes))*.01)))\n",
    "w_2 = theano.shared(np.asarray((np.random.randn(*(numHiddenNodes, numClasses))*.01)))\n",
    "params = [w_1, w_2]\n",
    "\n",
    "\n",
    "## (2) Model\n",
    "X = T.matrix()\n",
    "Y = T.matrix()\n",
    "# Two notes:\n",
    "# First, feed forward is the composition of layers (dot product + activation function)\n",
    "# Second, activation on the hidden layer still uses sigmoid\n",
    "def model(X, w_1, w_2):\n",
    "    return T.nnet.softmax(T.dot(T.nnet.sigmoid(T.dot(X, w_1)), w_2))\n",
    "y_hat = model(X, w_1, w_2)\n",
    "\n",
    "\n",
    "## (3) Cost...same as logistic regression\n",
    "cost = T.mean(T.nnet.categorical_crossentropy(y_hat, Y))\n",
    "\n",
    "\n",
    "## (4) Minimization.  Update rule changes to backpropagation.\n",
    "alpha = 0.01\n",
    "def backprop(cost, w):\n",
    "    grads = T.grad(cost=cost, wrt=w)\n",
    "    updates = []\n",
    "    for w1, grad in zip(w, grads):\n",
    "        updates.append([w1, w1 - grad * alpha])\n",
    "    return updates\n",
    "update = backprop(cost, params)\n",
    "train = theano.function(inputs=[X, Y], outputs=cost, updates=update, allow_input_downcast=True)\n",
    "y_pred = T.argmax(y_hat, axis=1)\n",
    "predict = theano.function(inputs=[X], outputs=y_pred, allow_input_downcast=True)\n",
    "\n",
    "miniBatchSize = 1\n",
    "def gradientDescentStochastic(epochs):\n",
    "    trainTime = 0.0\n",
    "    predictTime = 0.0\n",
    "    start_time = time.time()\n",
    "    for i in range(epochs):\n",
    "        for start, end in zip(range(0, len(scaled_train_data), miniBatchSize), range(miniBatchSize, len(scaled_train_data), miniBatchSize)):\n",
    "            cost = train(scaled_train_data[start:end], train_labels_b[start:end])\n",
    "        trainTime =  trainTime + (time.time() - start_time)\n",
    "        print('%d) accuracy = %.4f' %(i+1, np.mean(np.argmax(dev_labels_b, axis=1) == predict(scaled_dev_data))))\n",
    "    print('train time = %.2f' %(trainTime))\n",
    "\n",
    "gradientDescentStochastic(50)\n",
    "\n",
    "start_time = time.time()\n",
    "predict(scaled_dev_data)   \n",
    "print('predict time = %.2f' %(time.time() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next step: Use PCA to select feature"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
